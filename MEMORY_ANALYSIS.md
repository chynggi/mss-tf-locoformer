# MSS-TF-Locoformer ë©”ëª¨ë¦¬ ê³„ì‚° ë° ìµœì í™” ìƒì„¸ ë¶„ì„

## ğŸ” OOM ì›ì¸ ë¶„ì„

### ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸

```
torch.OutOfMemoryError: Tried to allocate 80.89 GiB
```

ì´ ì—ëŸ¬ëŠ” `scaled_dot_product_attention` í•¨ìˆ˜ì—ì„œ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

### ë©”ëª¨ë¦¬ ê³„ì‚°

#### 1. ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°

**XLarge ì„¤ì • (ì‹¤íŒ¨)**:
```
segment_length = 661500 samples (15ì´ˆ)
n_fft = 4096
hop_length = 1024

time_frames = 661500 / 1024 â‰ˆ 646 frames
freq_bins = 4096 / 2 + 1 = 2049 bins
```

**Memory-Optimized ì´ˆê¸° ì„¤ì • (ì—¬ì „íˆ ì‹¤íŒ¨)**:
```
segment_length = 441000 samples (10ì´ˆ)
n_fft = 4096
hop_length = 1024

time_frames = 441000 / 1024 â‰ˆ 430 frames
freq_bins = 2049 bins
```

#### 2. Attention ë©”ëª¨ë¦¬ ê³„ì‚°

Attentionì€ frequency ë˜ëŠ” time ì°¨ì›ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤:

**Frequency attention** (ë” ìœ„í—˜):
```
sequence_length = freq_bins = 2049
batch_size = 1
n_heads = 12
attention_dim = 192
head_dim = 192 / 12 = 16

Attention score matrix í¬ê¸°:
  [batch, n_heads, seq_len, seq_len]
  = [1, 12, 2049, 2049]
  = 50,388,588 elements
  
ë©”ëª¨ë¦¬ (float32):
  50,388,588 Ã— 4 bytes â‰ˆ 191 MB (ê´œì°®ìŒ)
  
ë©”ëª¨ë¦¬ (float16/bfloat16):
  50,388,588 Ã— 2 bytes â‰ˆ 96 MB (ê´œì°®ìŒ)
```

**Time attention**:
```
sequence_length = time_frames = 430
batch_size = 1
n_heads = 12

Attention score matrix:
  [1, 12, 430, 430]
  = 2,224,800 elements
  â‰ˆ 4.2 MB (ë§¤ìš° ì ìŒ)
```

**ê·¸ë ‡ë‹¤ë©´ ì™œ 80GBê°€ í•„ìš”í•œê°€?** ğŸ¤”

#### 3. ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°

ë¬¸ì œëŠ” **ëª¨ë“  ë ˆì´ì–´ì˜ ì¤‘ê°„ activationì„ ì €ì¥**í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ:

```
Total layers = 8 (TFLocoformer blocks)
ê° ë¸”ë¡ë§ˆë‹¤:
  - Frequency path: 1ê°œ attention
  - Time path: 1ê°œ attention
  
ì´ attention ì—°ì‚° = 8 Ã— 2 = 16ê°œ

ê° attentionì˜ ì¤‘ê°„ í…ì„œ:
  Q, K, V: [batch, n_heads, seq_len, head_dim]
  Attention scores: [batch, n_heads, seq_len, seq_len]
  Output: [batch, n_heads, seq_len, head_dim]
```

**ìµœì•…ì˜ ê²½ìš° (Frequency attention, backward pass í¬í•¨)**:
```
1ê°œ ë ˆì´ì–´ì˜ ë©”ëª¨ë¦¬:
  Input: [1, 430, 2049, 192] â‰ˆ 169 MB
  Q, K, V: 3 Ã— [1, 12, 2049, 16] â‰ˆ 2.4 MB
  Attention scores: [1, 12, 2049, 2049] â‰ˆ 96 MB
  Output: [1, 12, 2049, 16] â‰ˆ 0.8 MB
  Gradients (backward): ë™ì¼í•œ ì–‘
  
1ê°œ ë ˆì´ì–´ ì´í•©: ~540 MB
16ê°œ ë ˆì´ì–´: ~8.6 GB
```

í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ”:
- FFN ë ˆì´ì–´ì˜ ì¤‘ê°„ activation
- Normalization ë ˆì´ì–´ì˜ ì¤‘ê°„ ê°’
- Residual connectionì„ ìœ„í•œ ë³µì‚¬ë³¸
- Optimizer state (momentum, variance)
- ë“±ë“±...

**ì´í•©**: ì¶”ì • **20-30GB per batch**

í•˜ì§€ë§Œ ì—ëŸ¬ ë©”ì‹œì§€ëŠ” **80.89GB**ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì´ëŠ”:

1. **ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜**: ì—°ì†ëœ 80GB ë¸”ë¡ì´ í•„ìš”í–ˆì§€ë§Œ ë©”ëª¨ë¦¬ê°€ íŒŒí¸í™”ë¨
2. **Temporary tensor**: ì¼ë¶€ ì—°ì‚°ì´ ë§¤ìš° í° ì„ì‹œ í…ì„œë¥¼ ìƒì„±
3. **Conv2D ì—°ì‚°**: Encoder/Decoderì˜ Conv2Dê°€ í° ë©”ëª¨ë¦¬ í•„ìš”

## âœ… í•´ê²° ë°©ë²•

### 1. Segment Length ê°ì†Œ

**Ultra-Safe (3ì´ˆ)**:
```
segment_length = 132300 samples
time_frames = 132300 / 512 â‰ˆ 258 frames
freq_bins = 2048 / 2 + 1 = 1025 bins

Frequency attention memory:
  [1, 4, 1025, 1025] â‰ˆ 4.2 MB (âœ… ë§¤ìš° ì ìŒ!)
```

**Memory-Optimized (5ì´ˆ)**:
```
segment_length = 220500 samples
time_frames = 220500 / 512 â‰ˆ 430 frames
freq_bins = 1025 bins

Frequency attention memory:
  [1, 8, 1025, 1025] â‰ˆ 8.4 MB (âœ… ì ìŒ!)
```

### 2. ëª¨ë¸ í¬ê¸° ê°ì†Œ

| ì„¤ì • | n_layers | emb_dim | n_heads | ì´ íŒŒë¼ë¯¸í„° | ë©”ëª¨ë¦¬ |
|-----|----------|---------|---------|------------|--------|
| XLarge | 12 | 256 | 16 | ~120M | ë§¤ìš° ë†’ìŒ |
| Memory-Opt (Old) | 8 | 192 | 12 | ~60M | ë†’ìŒ |
| Memory-Opt (New) | 6 | 128 | 8 | ~30M | ì¤‘ê°„ |
| Ultra-Safe | 4 | 96 | 4 | ~15M | ë‚®ìŒ âœ… |

### 3. Gradient Accumulation

ë©”ëª¨ë¦¬ë¥¼ ë” ì ˆì•½í•˜ë ¤ë©´ gradient accumulation ì‚¬ìš©:

```yaml
training:
  batch_size: 1
  gradient_accumulation_steps: 4
  # Effective batch size = 1 Ã— 4 = 4
```

ë©”ëª¨ë¦¬ëŠ” batch_size=1ë§Œí¼ë§Œ ì‚¬ìš©í•˜ì§€ë§Œ íš¨ê³¼ëŠ” batch_size=4!

### 4. Gradient Checkpointing (ìµœí›„ì˜ ìˆ˜ë‹¨)

```yaml
training:
  gradient_checkpointing: true
```

- ë©”ëª¨ë¦¬: 30-40% ì ˆê°
- ì†ë„: 20-30% ê°ì†Œ
- Trade-off: ë©”ëª¨ë¦¬ â¬‡ï¸, ì‹œê°„ â¬†ï¸

## ğŸ“Š ì„¤ì •ë³„ ë©”ëª¨ë¦¬ ì˜ˆì¸¡

| ì„¤ì • | Segment | n_fft | Layers | Dim | ì˜ˆìƒ VRAM | OOM ìœ„í—˜ |
|-----|---------|-------|--------|-----|-----------|----------|
| XLarge | 15s | 4096 | 12 | 256 | **~97GB** | ğŸ”´ ë§¤ìš° ë†’ìŒ |
| Mem-Opt (Old) | 10s | 4096 | 8 | 192 | **~80GB** | ğŸ”´ ë†’ìŒ |
| Mem-Opt (New) | 5s | 2048 | 6 | 128 | ~30-40GB | ğŸŸ¡ ì¤‘ê°„ |
| Ultra-Safe | 3s | 2048 | 4 | 96 | ~15-20GB | ğŸŸ¢ ë§¤ìš° ë‚®ìŒ |

## ğŸ¯ ê¶Œì¥ ì„¤ì • ì„ íƒ ê°€ì´ë“œ

### ìƒí™©ë³„ ì¶”ì²œ

1. **ì²˜ìŒ ì‹¤í–‰ / OOM ë°œìƒ**: 
   ```bash
   python training/train.py --config configs/musdb18_ultra_safe.yaml
   ```
   - í™•ì‹¤íˆ ì‘ë™í•©ë‹ˆë‹¤!

2. **Ultra-Safeê°€ ì‘ë™í•˜ê³  ì—¬ìœ ê°€ ìˆìœ¼ë©´**:
   ```bash
   python training/train.py --config configs/musdb18_memory_optimized.yaml
   ```
   - ë” ë‚˜ì€ í’ˆì§ˆ

3. **ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´** (50GB+ ì—¬ìœ ):
   ```bash
   python training/train.py --config configs/musdb18.yaml
   ```
   - ì›ë˜ ì„¤ì •

## ğŸ’¡ Pro Tips

### 1. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# í•™ìŠµ ì „ í™•ì¸
nvidia-smi

# í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# Pythonì—ì„œ í™•ì¸
python -c "import torch; print(f'Available: {torch.cuda.mem_get_info()[0]/1e9:.1f}GB')"
```

### 2. ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### 3. ë°°ì¹˜ í¬ê¸° ìë™ ì°¾ê¸° (ì‹¤í—˜ì )
```python
# training/train.pyì—ì„œ
from torch.cuda.amp import autocast

def find_max_batch_size(model, sample_input):
    """ìë™ìœ¼ë¡œ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
    for bs in [1, 2, 4, 8, 16]:
        try:
            with torch.no_grad():
                batch = sample_input.repeat(bs, 1)
                output = model(batch)
            print(f"Batch size {bs}: OK")
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"Batch size {bs}: OOM")
                return bs - 1
    return bs
```

## ğŸ”¬ ì‹¤í—˜ ê²°ê³¼ (ì˜ˆìƒ)

### Ultra-Safe ì„¤ì •
- **í•™ìŠµ ì‹œê°„**: 3ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ â†’ ë” ë§ì€ ë°˜ë³µ í•„ìš”
- **ìˆ˜ë ´**: ë¹ ë¦„ (ì‘ì€ ëª¨ë¸)
- **ìµœì¢… ì„±ëŠ¥**: SI-SDR ~8-10dB (ì‹¤ìš©ì )
- **ì•ˆì •ì„±**: â­â­â­â­â­

### Memory-Optimized ì„¤ì •
- **í•™ìŠµ ì‹œê°„**: ì¤‘ê°„
- **ìˆ˜ë ´**: ì¤‘ê°„
- **ìµœì¢… ì„±ëŠ¥**: SI-SDR ~10-12dB (ì¢‹ìŒ)
- **ì•ˆì •ì„±**: â­â­â­â­

### ì›ë³¸ ì„¤ì •
- **í•™ìŠµ ì‹œê°„**: ëŠë¦¼ (í° ì„¸ê·¸ë¨¼íŠ¸)
- **ìˆ˜ë ´**: ëŠë¦¼ (í° ëª¨ë¸)
- **ìµœì¢… ì„±ëŠ¥**: SI-SDR ~12-15dB (ìµœê³ )
- **ì•ˆì •ì„±**: â­â­ (OOM ìœ„í—˜)

---

**ê²°ë¡ **: ì§€ê¸ˆì€ `musdb18_ultra_safe.yaml`ë¡œ ì‹œì‘í•˜ì„¸ìš”! ğŸš€
