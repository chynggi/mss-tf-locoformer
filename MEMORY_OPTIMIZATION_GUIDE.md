# VRAM ìµœì í™” ê°€ì´ë“œ (RTX PRO 6000 - 97GB)

## ğŸš¨ í˜„ì¬ ë¬¸ì œ

VRAM ì‚¬ìš©ëŸ‰ì´ 96.6GB / 97.9GB (98.7%)ë¡œ í•œê³„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.

## âœ… ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•´ê²°ì±…

### 1. **Gradient Accumulation** (ê°€ì¥ íš¨ê³¼ì )

ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  gradient accumulationìœ¼ë¡œ ë³´ì™„:

```yaml
# configs/musdb18_memory_optimized.yaml ì‚¬ìš©
training:
  batch_size: 1  # ìµœì†Œí™”
  gradient_accumulation_steps: 8  # Effective batch = 8
```

**ì˜ˆìƒ íš¨ê³¼**: VRAM 50-60% ì ˆê°

### 2. **Segment Length ì¤„ì´ê¸°**

ë” ì§§ì€ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ìš©:

```yaml
dataset:
  segment_length: 441000  # 10ì´ˆ (15ì´ˆì—ì„œ ê°ì†Œ)
```

**ì˜ˆìƒ íš¨ê³¼**: VRAM 30-40% ì ˆê°

### 3. **ëª¨ë¸ í¬ê¸° ì¶•ì†Œ**

```yaml
model:
  n_layers: 8  # 12 â†’ 8
  emb_dim: 192  # 256 â†’ 192
  n_heads: 12  # 16 â†’ 12
  ffn_hidden_dim: [768, 768]  # [1024, 1024] â†’ [768, 768]
```

**ì˜ˆìƒ íš¨ê³¼**: VRAM 20-30% ì ˆê°

### 4. **Gradient Checkpointing** (ì¶”ê°€ ìµœì í™”)

ì†ë„ë¥¼ í¬ìƒí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½:

```yaml
training:
  gradient_checkpointing: true
```

**ì˜ˆìƒ íš¨ê³¼**: VRAM ì¶”ê°€ 30-40% ì ˆê° (ì†ë„ 20-30% ê°ì†Œ)

## ğŸ”§ ì ìš© ë°©ë²•

### ì¦‰ì‹œ ì¬ì‹œì‘ (ê¶Œì¥)

1. **í˜„ì¬ í•™ìŠµ ì¤‘ë‹¨**:
```bash
# Ctrl+Cë¡œ í•™ìŠµ ì¤‘ë‹¨
```

2. **ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œì‘**:
```bash
python training/train.py --config configs/musdb18_memory_optimized.yaml
```

### ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

```bash
python training/train.py \
  --config configs/musdb18_memory_optimized.yaml \
  --resume experiments_xlarge/checkpoints/best_model.pth
```

## ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ VRAM ì‚¬ìš©ëŸ‰ì´ progress barì— í‘œì‹œë©ë‹ˆë‹¤:

```
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [10:23<00:00, loss=2.3456, lr=0.000300, mem=45.2/50.1GB]
                                                                        ^^^^^^^^^^^^^^^^
```

## ğŸ¯ ìµœì  ì„¤ì • ì¶”ì²œ

í˜„ì¬ ìƒí™©ì—ì„œëŠ” ë‹¤ìŒ ì„¤ì •ì´ ìµœì ì…ë‹ˆë‹¤:

| íŒŒë¼ë¯¸í„° | ê¸°ì¡´ (XLarge) | ìµœì í™” | ì´ìœ  |
|---------|--------------|--------|------|
| batch_size | 8 | 1 | VRAM ëŒ€í­ ì ˆê° |
| gradient_accum | 2 | 8 | Effective batch ìœ ì§€ |
| segment_length | 661500 (15s) | 441000 (10s) | ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ |
| n_layers | 12 | 8 | ëª¨ë¸ ê¹Šì´ ê°ì†Œ |
| emb_dim | 256 | 192 | ì°¨ì› ê°ì†Œ |
| n_heads | 16 | 12 | í—¤ë“œ ìˆ˜ ê°ì†Œ |

## ğŸ” ì¶”ê°€ ë””ë²„ê¹…

### 1. í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
nvidia-smi
```

### 2. PyTorch ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

í•™ìŠµ ì½”ë“œì— ì¶”ê°€:

```python
import torch

# ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
print(torch.cuda.memory_summary())

# í• ë‹¹ëœ ë©”ëª¨ë¦¬
allocated = torch.cuda.memory_allocated() / 1024**3
reserved = torch.cuda.memory_reserved() / 1024**3
print(f"Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
```

### 3. OOM ë°œìƒ ì‹œ

```bash
# ë”ìš± ë³´ìˆ˜ì ì¸ ì„¤ì •
python training/train.py --config configs/musdb18_small.yaml
```

## ğŸ’¡ ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„

| ì„¤ì • | VRAM | í•™ìŠµ ì†ë„ | ëª¨ë¸ í’ˆì§ˆ |
|-----|------|----------|----------|
| XLarge | ~97GB | ë¹ ë¦„ | ìµœê³  |
| Memory-Optimized | ~45-50GB | ì¤‘ê°„ | ë†’ìŒ |
| Small | ~20-25GB | ëŠë¦¼ | ì¤‘ê°„ |

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ ëª…ë ¹ì–´

### Option 1: ë©”ëª¨ë¦¬ ìµœì í™” (ì¶”ì²œ)
```bash
python training/train.py --config configs/musdb18_memory_optimized.yaml
```

### Option 2: Small ì„¤ì • (ì•ˆì „)
```bash
python training/train.py --config configs/musdb18_small.yaml
```

### Option 3: ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì´ì–´ì„œ (ì£¼ì˜: ì—¬ì „íˆ OOM ê°€ëŠ¥)
```bash
python training/train.py \
  --config configs/musdb18_memory_optimized.yaml \
  --resume experiments_xlarge/checkpoints/checkpoint_epoch*.pth
```

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼

**ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì ìš© í›„**:
- VRAM ì‚¬ìš©ëŸ‰: 96.6GB â†’ **45-50GB** (ì•½ 50% ê°ì†Œ)
- Effective batch size: 16 â†’ **8** (ìœ ì§€ ê°€ëŠ¥)
- í•™ìŠµ ì†ë„: 100% â†’ **70-80%** (gradient accumulation ì˜¤ë²„í—¤ë“œ)
- ëª¨ë¸ í’ˆì§ˆ: ê±°ì˜ ë™ì¼ (ì•½ê°„ ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ ì¶©ë¶„í•œ ìš©ëŸ‰)

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **Gradient Accumulation**: optimizer stepì´ Në²ˆì— í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë¯€ë¡œ í•™ìŠµ ì†ë„ê°€ ì•½ê°„ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **Gradient Checkpointing**: í™œì„±í™”í•˜ë©´ forward passë¥¼ ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ í•™ìŠµ ì‹œê°„ì´ 20-30% ì¦ê°€í•©ë‹ˆë‹¤. VRAMì´ ì—¬ì „íˆ ë¶€ì¡±í•  ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

3. **ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±**: ëª¨ë¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì¬í•™ìŠµí•˜ê±°ë‚˜ ë™ì¼í•œ êµ¬ì¡° ìœ ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ğŸ”„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ:

```bash
# 1ì´ˆë§ˆë‹¤ GPU ìƒíƒœ í™•ì¸
watch -n 1 nvidia-smi

# ë˜ëŠ” ë” ìƒì„¸í•œ ì •ë³´
nvidia-smi dmon -s u
```

## ğŸ“ ë¬¸ì œ í•´ê²°

ì—¬ì „íˆ OOM ë°œìƒ ì‹œ:

1. `segment_length`ë¥¼ ë” ì¤„ì´ê¸°: 441000 â†’ 220500 (5ì´ˆ)
2. `n_layers`ë¥¼ ë” ì¤„ì´ê¸°: 8 â†’ 6 ë˜ëŠ” 4
3. `emb_dim`ì„ ë” ì¤„ì´ê¸°: 192 â†’ 128
4. `gradient_checkpointing: true` í™œì„±í™”

---

**ì¦‰ì‹œ ì‹¤í–‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤:**

```bash
# í˜„ì¬ í•™ìŠµ ì¤‘ë‹¨ (Ctrl+C)
# ê·¸ë¦¬ê³ :
python training/train.py --config configs/musdb18_memory_optimized.yaml
```
